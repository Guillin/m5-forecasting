{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas import Series\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../../../data/processed'\n",
    "INPUT_FILE_NAME = 'dataproc_v001'\n",
    "OUTPUT_PATH = '../../../models/arima/hyperparameters/'\n",
    "HYPERPARAM_NAME = 'best_hyperparam_arima_r'\n",
    "LOG_NAME = 'gsearch_arima_logs_r'\n",
    "OUTPUT_FILE_NAME = 'gsearch_arima_logs_d'\n",
    "NRUN = 1\n",
    "DAYS_PRED = 28\n",
    "METRIC = 'rmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(train, test, arima_order, metric='rmse'):\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    n_test = test.shape[0]\n",
    "    \n",
    "    for test in test:\n",
    "  \n",
    "        history = [x for x in train]\n",
    "        # make predictions\n",
    "        predictions = list()\n",
    "        for t in test:\n",
    "            model = ARIMA(history, order=arima_order)\n",
    "            model_fit = model.fit(disp=0)\n",
    "            yhat = model_fit.forecast()[0]\n",
    "            predictions.append(yhat)\n",
    "            history.append(t)\n",
    "\n",
    "        # calculate out of sample error\n",
    "        if metric=='mse':\n",
    "            error += mean_squared_error(test, predictions)\n",
    "        elif metric == 'rmse':\n",
    "            error += rmse(test, predictions)\n",
    "\n",
    "\n",
    "\n",
    "    return error/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsearch_arima_models(train, test, p_values, d_values, q_values, metric='rmse'):\n",
    "    train = train.astype('float32')\n",
    "    test = test.astype('float32')\n",
    "    \n",
    "    \n",
    "    \n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                \n",
    "                order = (p,d,q)\n",
    "                line = datetime.now().strftime(\"%d/%m/%Y\") + \", \"\n",
    "                \n",
    "                # open log file\n",
    "                f = open(f'{OUTPUT_PATH}/{LOG_NAME}{NRUN}.csv','a+')\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    score = evaluate_arima_model(train, test, order)\n",
    "                    \n",
    "                    if score < best_score:\n",
    "                        best_score, best_cfg = score, order\n",
    "                    \n",
    "                    print('ARIMA%s %s=%.3f' % (order, metric, score))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                line += str(order[0]) + \", \" + str(order[1]) + \", \" + str(order[2]) + \", \" + METRIC + \", \" + str(score) + \"\\n\"\n",
    "\n",
    "                \n",
    "                # save into log file\n",
    "                with open(f'{OUTPUT_PATH}/{LOG_NAME}{NRUN}.csv','a+') as f:\n",
    "                    f.write(line)\n",
    "                \n",
    "                \n",
    "    print('Best ARIMA%s %s=%.3f' % (best_cfg, metric,best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maddest(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
    "\n",
    "def denoise_signal(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_smoothing(signal, kernel_size=3, stride=1):\n",
    "    sample = []\n",
    "    start = 0\n",
    "    end = kernel_size\n",
    "    while end <= len(signal):\n",
    "        start = start + stride\n",
    "        end = end + stride\n",
    "        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n",
    "    return np.array(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(f'{INPUT_PATH}/{INPUT_FILE_NAME}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.part == 'train'] # select only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cutoff = data.d.max() - DAYS_PRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[data.d <= date_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data[data.d > date_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = ['state_id', 'd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg = X_train.groupby(LEVEL).demand.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUTPUT_PATH}/{LOG_NAME}{NRUN}.csv','w+') as f:\n",
    "    f.write(f\"date, p, d, q, metric, score\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "d_values = range(0, 3) #this is the range for the values\n",
    "q_values = range(0, 3) #this is the range for the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 0\n",
    "train = train_agg[train_agg.state_id == ID].demand.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_test[X_test.state_id == ID].pivot(index='id', columns='d', values='demand').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridx = np.random.choice(nrows, size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[ridx, :] # select only some rows to test because take to many time to evaluate the model"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "statenv",
   "language": "python",
   "name": "statenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
