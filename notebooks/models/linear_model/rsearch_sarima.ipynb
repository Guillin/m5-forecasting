{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import Series\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../../../data/processed'\n",
    "INPUT_FILE_NAME = 'dataproc_v001'\n",
    "OUTPUT_PATH = '../../../models/arima/hyperparameters/'\n",
    "HYPERPARAM_NAME = 'best_hyperparam_sarima_r'\n",
    "LOG_NAME = 'rsearch_arima_logs_r'\n",
    "OUTPUT_FILE_NAME = 'rsearch_sarima_logs_d'\n",
    "NITER = 200\n",
    "NRUN = 3\n",
    "DAYS_PRED = 28\n",
    "METRIC = 'rmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-step sarima forecast\n",
    "def sarima_forecast(history, config):\n",
    "    order, sorder, trend = config\n",
    "    # define model\n",
    "    model = SARIMAX(history, order=order, seasonal_order=sorder, \n",
    "                    trend=trend, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    # fit model\n",
    "    model_fit = model.fit(disp=False)\n",
    "    # make one step forecast\n",
    "    yhat = model_fit.predict(len(history), len(history))\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(train, test, cfg, metric='rmse'):\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    n_test = test.shape[0]\n",
    "    \n",
    "    for test in test:\n",
    "  \n",
    "        history = [x for x in train]\n",
    "        # make predictions\n",
    "        predictions = list()\n",
    "        for t in test:           \n",
    "            # fit model and make forecast for history\n",
    "            yhat = sarima_forecast(history, cfg)\n",
    "            predictions.append(yhat)\n",
    "            history.append(t)\n",
    "\n",
    "        # calculate out of sample error\n",
    "        if metric=='mse':\n",
    "            error += mean_squared_error(test, predictions)\n",
    "        elif metric == 'rmse':\n",
    "            error += rmse(test, predictions)\n",
    "\n",
    "\n",
    "\n",
    "    return error/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score a model, return None on failure\n",
    "def score_model(train, test, cfg, debug=False):\n",
    "    result = None\n",
    "    # convert config to a key\n",
    "    key = str(cfg)\n",
    "    # show all warnings and fail on exception if debugging\n",
    "    if debug:\n",
    "        result = walk_forward_validation(train, test, cfg)\n",
    "    else:\n",
    "        # one failure during model validation suggests an unstable config\n",
    "        try:\n",
    "            # never show warnings when grid searching, too noisy\n",
    "            with catch_warnings():\n",
    "                filterwarnings(\"ignore\")\n",
    "                tic = datetime.now()\n",
    "                result = walk_forward_validation(train, test, cfg)\n",
    "                toc = datetime.now()\n",
    "        except:\n",
    "            result = None\n",
    "    # check for an interesting result\n",
    "    if result is not None:\n",
    "        #print(' > Model[%s] %.3f' % (key, result))\n",
    "        diff_time_run = toc - tic\n",
    "        line = datetime.now().strftime(\"%d/%m/%Y\") + \", \" + str(key) + \", \" + METRIC + \", \" + str(result) + \", \" + str(diff_time_run.total_seconds()/60) + \"\\n\"\n",
    "        \n",
    "        # save into log file\n",
    "        with open(f'{OUTPUT_PATH}/{LOG_NAME}{NRUN}.csv','a+') as f:\n",
    "            f.write(line)\n",
    "        \n",
    "    return (key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search configs\n",
    "def grid_search(train, test, cfg_list, parallel=True):\n",
    "    scores = None\n",
    "    if parallel:\n",
    "        # execute configs in parallel\n",
    "        executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n",
    "        tasks = (delayed(score_model)(train, test, cfg) for cfg in cfg_list)\n",
    "        scores = executor(tasks)\n",
    "    else:\n",
    "        scores = [score_model(train, test, cfg) for cfg in cfg_list]\n",
    "    # remove empty results\n",
    "    scores = [r for r in scores if r[1] != None]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of sarima configs to try\n",
    "def sarima_configs(seasonal=[0]):\n",
    "\tmodels = list()\n",
    "\t# define config lists\n",
    "\tp_params = [i for i in range(11)]\n",
    "\td_params = [i for i in range(4)]\n",
    "\tq_params = [i for i in range(4)]\n",
    "\tt_params = ['n','c','t','ct']\n",
    "\tP_params = [i for i in range(4)]\n",
    "\tD_params = [0, 1, 2]\n",
    "\tQ_params = [i for i in range(4)]\n",
    "\tm_params = seasonal\n",
    "\t# create config instances\n",
    "\tfor p in p_params:\n",
    "\t\tfor d in d_params:\n",
    "\t\t\tfor q in q_params:\n",
    "\t\t\t\tfor t in t_params:\n",
    "\t\t\t\t\tfor P in P_params:\n",
    "\t\t\t\t\t\tfor D in D_params:\n",
    "\t\t\t\t\t\t\tfor Q in Q_params:\n",
    "\t\t\t\t\t\t\t\tfor m in m_params:\n",
    "\t\t\t\t\t\t\t\t\tcfg = [(p,d,q), (P,D,Q,m), t]\n",
    "\t\t\t\t\t\t\t\t\tmodels.append(cfg)\n",
    "\treturn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(f'{INPUT_PATH}/{INPUT_FILE_NAME}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.part == 'train'] # select only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cutoff = data.d.max() - DAYS_PRED\n",
    "\n",
    "X_train = data[data.d <= date_cutoff]\n",
    "\n",
    "X_test = data[data.d > date_cutoff]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = ['state_id', 'd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg = X_train.groupby(LEVEL).demand.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUTPUT_PATH}/{LOG_NAME}{NRUN}.csv','w+') as f:\n",
    "    f.write(f\"date, params, metric, score, time [min] \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configs\n",
    "cfg_list = sarima_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random parameters \n",
    "ncfg = len(cfg_list)\n",
    "rcfg = np.random.choice(ncfg, size=NITER, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cfg_list =  np.array(cfg_list)[rcfg,:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 2\n",
    "train = train_agg[train_agg.state_id == ID].demand.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_test[X_test.state_id == ID].pivot(index='id', columns='d', values='demand').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridx = np.random.choice(nrows, size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[ridx, :] # select only some rows to test because take to many time to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "scores = grid_search(train, test, random_cfg_list)\n",
    "print('done')\n",
    "# list top 3 configs\n",
    "for cfg, error in scores[:3]:\n",
    "    print(cfg, error)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "statenv",
   "language": "python",
   "name": "statenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
