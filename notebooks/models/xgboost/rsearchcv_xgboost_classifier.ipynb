{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RANDOM SEARCH CV - XGBOOST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '../../../data/train_test/'\n",
    "MODELPATH = '../../../models/xgboost/'\n",
    "SEED = 47\n",
    "NITER = 100\n",
    "CV = 3\n",
    "SCORE = 'roc_auc'\n",
    "handlingnull = False\n",
    "NJOBS = -1\n",
    "NTHREADS= 4\n",
    "USEGPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_pickle(DATAPATH+'X_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_pickle(DATAPATH+'y_train.pkl')['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DMatrix and handling Null values\n",
    "if handlingnull:\n",
    "    xgtrain = xgb.DMatrix(train_features, train_labels, missing=-9999)\n",
    "else:\n",
    "    xgtrain = xgb.DMatrix(train_features.values, train_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET UP HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[xgboost params](https://xgboost.readthedocs.io/en/latest/python/python_api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "booster = 'gbtree'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = [0.01] \n",
    "\n",
    "# Defines the minimum sum of weights of all observations required in a child.\n",
    "min_child_weight = [i for i in range(1,10,2)]\n",
    "\n",
    "# The maximum depth of a tree\n",
    "max_depth = [i for i in range(3,10,2)] \n",
    "\n",
    "# A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "# Gamma specifies the minimum loss reduction required to make a split.\n",
    "gamma = [i/10.0 for i in range(0,5)]\n",
    "\n",
    "# Denotes the fraction of observations to be randomly samples for each tree.\n",
    "subsample = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "# Denotes the fraction of columns to be randomly samples for each tree.\n",
    "colsample_bytree = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "# L2 regularization term on weights (analogous to Ridge regression)\n",
    "reg_lambda = [i/10.0 for i in range(4,10)]\n",
    "\n",
    "# L1 regularization term on weight (analogous to Lasso regression)\n",
    "reg_alpha = [0, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = int((len(train_labels) - np.sum(train_labels.values))/np.sum(train_labels.values))\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "\n",
    "# This defines the loss function to be minimized. \n",
    "# - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "# - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "#   you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "# - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "objective  = 'binary:logistic'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse – root mean square error\n",
    "# - mae – mean absolute error\n",
    "# - logloss – negative log-likelihood\n",
    "# - error – Binary classification error rate (0.5 threshold)\n",
    "# - merror – Multiclass classification error rate\n",
    "# - mlogloss – Multiclass logloss\n",
    "# - auc: Area under the curve\n",
    "eval_metric = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET UP RANDOM SEARCH GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'learning_rate' : eta,\n",
    "    'min_child_weight' : min_child_weight,\n",
    "    'max_depth' : max_depth,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'reg_lambda' : reg_lambda,\n",
    "    'reg_alpha' : reg_alpha,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND NUM BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=XGBClassifier(seed=SEED, booster=booster, objective=objective,  scale_pos_weight = scale_pos_weight, nthread=NJOBS)\n",
    "xgb_param = model.get_xgb_params()\n",
    "xgb_param['objective'] = objective\n",
    "\n",
    "if USEGPU:\n",
    "    xgb_param['tree_method'] = 'gpu_hist'\n",
    "    xgb_param['gpu_id'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round = 1000, nfold = CV, metrics = eval_metric, early_stopping_rounds = early_stopping_rounds, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best number of boosters: \", n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET UP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model = XGBClassifier(n_estimators=n_estimators, scale_pos_weight=scale_pos_weight,  objective=objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USEGPU:\n",
    "    model.set_params(gpu_id = 0)\n",
    "    model.set_params(tree_method='gpu_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using CV fold cross validation, \n",
    "# search across NITER different combinations, and use all available cores\n",
    "xgboost_rsearch = RandomizedSearchCV(estimator = model, \n",
    "                                     param_distributions = random_grid, \n",
    "                                     scoring=SCORE, \n",
    "                                     n_iter = NITER, \n",
    "                                     cv = CV, verbose=2, \n",
    "                                     random_state=SEED, \n",
    "                                     n_jobs = NJOBS)# Fit the random search model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCH BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed: 135.2min\n",
      "[Parallel(n_jobs=7)]: Done 148 tasks      | elapsed: 608.9min\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "xgboost_rsearch.fit(train_features, train_labels)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), NITER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgboost_rsearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = xgboost_rsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params['n_estimators'] = n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best cross validation score (' + SCORE,'): ', xgboost_rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE HYPERPARAMETERS AND RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(xgboost_rsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.to_csv(MODELPATH + 'result/rsearch_xgboost_classifier_d' + str(datetime.now().date()) + '.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(MODELPATH + 'hyperparameter/rseach_xgboost_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(MODELPATH + 'result/rseach_xgboost_classifier_best_estimator_d' + str(datetime.now().date()) + '.npy', xgboost_rsearch.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (boostenv)",
   "language": "python",
   "name": "boostenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
