{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LigthGBM - CLASSIFICATION - BAYESIAN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/train_test/'\n",
    "SEED = 47\n",
    "NITER = 100\n",
    "CV = 3\n",
    "SCORE = 'roc_auc'\n",
    "handlingnull = False\n",
    "NJOBS = -1\n",
    "USEGPU = False\n",
    "NCLASS = 3 # number class to predict (if bivar set 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_pickle(DATAPATH+'X_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_pickle(DATAPATH+'y_train.pkl')['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_eng = np.load('data/train_test/features_selected.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features[features_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865, 800)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if handlingnull:\n",
    "    train_features[np.isnan(train_features)] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "boosting = 'gbdt'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = [0.01] \n",
    "\n",
    "\n",
    "# A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "# Gamma specifies the minimum loss reduction required to make a split.\n",
    "gamma = [i/10.0 for i in range(0,5)]\n",
    "\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = (len(train_labels) - sum(train_labels))/sum(train_labels)\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "# This defines the loss function to be minimized. See documentation\n",
    "# -  options: regression, regression_l1, huber, fair, poisson, quantile, \n",
    "# mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda,\n",
    "# lambdarank, aliases: objective_type, app, application\n",
    "objective  = 'binary'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse, root square loss, aliases: root_mean_squared_error, l2_root\n",
    "# - quantile, Quantile regression\n",
    "# - mape, MAPE loss, aliases: mean_absolute_percentage_error\n",
    "# - huber, Huber loss\n",
    "# - fair, Fair loss\n",
    "# - poisson, negative log-likelihood for Poisson regression\n",
    "# - gamma, negative log-likelihood for Gamma regression\n",
    "# - gamma_deviance, residual deviance for Gamma regression\n",
    "# - tweedie, negative log-likelihood for Tweedie regression\n",
    "# - ndcg, NDCG, aliases: lambdarank\n",
    "# - map, MAP, aliases: mean_average_precision\n",
    "# - auc, AUC\n",
    "# - binary_logloss, log loss, aliases: binary\n",
    "metric = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[lightGBM params](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "help(lgb.LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "# Domain space-- Range of hyperparameters\n",
    "pds = {\n",
    "    # Minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "    # default = 20, type = int, aliases: min_data_per_leaf, min_data, min_child_samples, \n",
    "    'num_leaves': (20, 100),\n",
    "\n",
    "    # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    'feature_fraction': (0.1, 0.9),\n",
    "    \n",
    "    # Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    'bagging_fraction': (0.8, 1),\n",
    "\n",
    "    # The maximum depth of a tree\n",
    "    'max_depth': (9, 13 ),\n",
    "\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "\n",
    "    # Minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "    # Default 1e-3\n",
    "    'min_child_weight': (30, 50),\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': (20, 100),\n",
       " 'feature_fraction': (0.1, 0.9),\n",
       " 'bagging_fraction': (0.8, 1),\n",
       " 'max_depth': (9, 13),\n",
       " 'min_split_gain': (0.001, 0.1),\n",
       " 'min_child_weight': (30, 50)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find num boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "lgb_params = {\n",
    "    'boosting_type': boosting,\n",
    "    'objective': objective,\n",
    "    'metric': metric,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_threads' : 8,\n",
    "    'verbose': 0,\n",
    "    #'num_class':  NCLASS,\n",
    "    'seed' : SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = lgb.cv(lgb_params, lgb_train, num_boost_round = 1000, nfold = CV, metrics = metric, early_stopping_rounds = early_stopping_rounds, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = pd.DataFrame(cvresult).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators found:  344\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of estimators found: \", n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight):\n",
    "      \n",
    "    params = {'boosting_type': boosting,\n",
    "              'application': objective,\n",
    "              'num_iterations': n_estimators,\n",
    "              'learning_rate':eta, \n",
    "              'early_stopping_round':50,\n",
    "              'metric': metric} # Default parameters\n",
    "    \n",
    "    \n",
    "    params[\"num_leaves\"] = int(round(num_leaves))\n",
    "    params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['min_split_gain'] = min_split_gain\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['min_split_gain'] = min_split_gain\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_result = lgb.cv(params, lgb_train, nfold=CV, seed=SEED, stratified=False, verbose_eval=None, metrics = metric)\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = max(cv_result['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    #loss = 1 - best_score\n",
    "\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate model\n",
    "optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_sp... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 0.8153  \u001b[0m | \u001b[0m 0.7239  \u001b[0m | \u001b[0m 10.75   \u001b[0m | \u001b[0m 44.47   \u001b[0m | \u001b[0m 0.09782 \u001b[0m | \u001b[0m 63.08   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7922  \u001b[0m | \u001b[0m 0.9002  \u001b[0m | \u001b[0m 0.1576  \u001b[0m | \u001b[0m 10.07   \u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 0.06824 \u001b[0m | \u001b[0m 84.3    \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7906  \u001b[0m | \u001b[0m 0.8762  \u001b[0m | \u001b[0m 0.1527  \u001b[0m | \u001b[0m 10.15   \u001b[0m | \u001b[0m 48.19   \u001b[0m | \u001b[0m 0.02213 \u001b[0m | \u001b[0m 56.17   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7886  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 11.4    \u001b[0m | \u001b[0m 49.0    \u001b[0m | \u001b[0m 0.0238  \u001b[0m | \u001b[0m 63.88   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7931  \u001b[0m | \u001b[0m 0.9818  \u001b[0m | \u001b[0m 0.2065  \u001b[0m | \u001b[0m 11.09   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 0.06723 \u001b[0m | \u001b[0m 57.42   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7913  \u001b[0m | \u001b[0m 0.811   \u001b[0m | \u001b[0m 0.897   \u001b[0m | \u001b[0m 12.87   \u001b[0m | \u001b[0m 30.29   \u001b[0m | \u001b[0m 0.008335\u001b[0m | \u001b[0m 20.08   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7955  \u001b[0m | \u001b[0m 0.8867  \u001b[0m | \u001b[0m 0.8468  \u001b[0m | \u001b[0m 9.032   \u001b[0m | \u001b[0m 30.22   \u001b[0m | \u001b[0m 0.09413 \u001b[0m | \u001b[0m 55.01   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.7979  \u001b[0m | \u001b[95m 0.849   \u001b[0m | \u001b[95m 0.8811  \u001b[0m | \u001b[95m 12.96   \u001b[0m | \u001b[95m 30.0    \u001b[0m | \u001b[95m 0.09479 \u001b[0m | \u001b[95m 98.65   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7913  \u001b[0m | \u001b[0m 0.8445  \u001b[0m | \u001b[0m 0.8907  \u001b[0m | \u001b[0m 9.169   \u001b[0m | \u001b[0m 49.03   \u001b[0m | \u001b[0m 0.05483 \u001b[0m | \u001b[0m 21.08   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.797   \u001b[0m | \u001b[0m 0.8631  \u001b[0m | \u001b[0m 0.8789  \u001b[0m | \u001b[0m 12.96   \u001b[0m | \u001b[0m 30.46   \u001b[0m | \u001b[0m 0.09242 \u001b[0m | \u001b[0m 68.96   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 0.8854  \u001b[0m | \u001b[0m 9.134   \u001b[0m | \u001b[0m 30.67   \u001b[0m | \u001b[0m 0.0562  \u001b[0m | \u001b[0m 96.0    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7974  \u001b[0m | \u001b[0m 0.9732  \u001b[0m | \u001b[0m 0.8924  \u001b[0m | \u001b[0m 12.98   \u001b[0m | \u001b[0m 30.48   \u001b[0m | \u001b[0m 0.07528 \u001b[0m | \u001b[0m 79.25   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7978  \u001b[0m | \u001b[0m 0.8532  \u001b[0m | \u001b[0m 0.834   \u001b[0m | \u001b[0m 12.87   \u001b[0m | \u001b[0m 30.39   \u001b[0m | \u001b[0m 0.07232 \u001b[0m | \u001b[0m 99.09   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7976  \u001b[0m | \u001b[0m 0.801   \u001b[0m | \u001b[0m 0.8291  \u001b[0m | \u001b[0m 12.98   \u001b[0m | \u001b[0m 30.1    \u001b[0m | \u001b[0m 0.04228 \u001b[0m | \u001b[0m 82.41   \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.7979  \u001b[0m | \u001b[95m 0.8327  \u001b[0m | \u001b[95m 0.8882  \u001b[0m | \u001b[95m 12.57   \u001b[0m | \u001b[95m 30.09   \u001b[0m | \u001b[95m 0.004146\u001b[0m | \u001b[95m 99.31   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7961  \u001b[0m | \u001b[0m 0.8228  \u001b[0m | \u001b[0m 0.8956  \u001b[0m | \u001b[0m 9.012   \u001b[0m | \u001b[0m 30.26   \u001b[0m | \u001b[0m 0.003925\u001b[0m | \u001b[0m 83.84   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7979  \u001b[0m | \u001b[0m 0.9469  \u001b[0m | \u001b[0m 0.8418  \u001b[0m | \u001b[0m 12.86   \u001b[0m | \u001b[0m 30.36   \u001b[0m | \u001b[0m 0.06358 \u001b[0m | \u001b[0m 99.56   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7976  \u001b[0m | \u001b[0m 0.8743  \u001b[0m | \u001b[0m 0.8969  \u001b[0m | \u001b[0m 12.8    \u001b[0m | \u001b[0m 30.1    \u001b[0m | \u001b[0m 0.06823 \u001b[0m | \u001b[0m 83.37   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.7977  \u001b[0m | \u001b[0m 0.8812  \u001b[0m | \u001b[0m 0.8953  \u001b[0m | \u001b[0m 12.98   \u001b[0m | \u001b[0m 30.13   \u001b[0m | \u001b[0m 0.06559 \u001b[0m | \u001b[0m 85.68   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7977  \u001b[0m | \u001b[0m 0.8508  \u001b[0m | \u001b[0m 0.8891  \u001b[0m | \u001b[0m 12.67   \u001b[0m | \u001b[0m 30.85   \u001b[0m | \u001b[0m 0.0984  \u001b[0m | \u001b[0m 99.54   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "optimizer.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params['n_estimators'] = n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/models/bayesianoptcv_gbm_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xgboost (env)",
   "language": "python",
   "name": "xgboostenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
