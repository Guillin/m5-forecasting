{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LigthGBM - RANDOM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/processed/'\n",
    "SEED = 47\n",
    "NITER = 100\n",
    "CV = 3\n",
    "SCORE = 'roc_auc'\n",
    "handlingnull = False\n",
    "NJOBS = -1\n",
    "USEGPU = False\n",
    "NCLASS = 3 # number class to predict (if bivar set 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(DATAPATH+'X.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_pickle(DATAPATH+'y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865, 1770)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle(DATAPATH+'submission.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign = pd.read_pickle('data/features/campaign_quarter_001.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital = pd.read_pickle('data/features/digital_features_period_001.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if handlingnull:\n",
    "    train_features[np.isnan(train_features)] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "boosting = 'gbdt'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = [0.01] \n",
    "\n",
    "# Minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "# Default 1e-3\n",
    "min_child_weight = [0.001, 0.005, 0.01, 0.05, 0.1, 1]\n",
    "\n",
    "# Minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "# default = 20, type = int, aliases: min_data_per_leaf, min_data, min_child_samples, \n",
    "# constraints: min_data_in_leaf >= 0\n",
    "min_data_in_leaf = [i for i in range(20,1000,40)]\n",
    "\n",
    "# The maximum depth of a tree\n",
    "max_depth = [i for i in range(3,10,2)] \n",
    "\n",
    "# A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "# Gamma specifies the minimum loss reduction required to make a split.\n",
    "gamma = [i/10.0 for i in range(0,5)]\n",
    "\n",
    "# Denotes the fraction of observations to be randomly samples for each tree.\n",
    "subsample = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "# Denotes the fraction of columns to be randomly samples for each tree.\n",
    "colsample_bytree = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "# frequency for bagging\n",
    "# 0 means disable bagging; k means perform bagging at every k iteration\n",
    "# Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n",
    "bagging_freq = [3, 5, 10, 20, 30]\n",
    "\n",
    "# L2 regularization term on weights (analogous to Ridge regression)\n",
    "reg_lambda = [i/10.0 for i in range(4,10)]\n",
    "\n",
    "# L1 regularization term on weight (analogous to Lasso regression)\n",
    "reg_alpha = [0, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = (len(train_labels) - sum(train_labels))/sum(train_labels)\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "# This defines the loss function to be minimized. See documentation\n",
    "# -  options: regression, regression_l1, huber, fair, poisson, quantile, \n",
    "# mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda,\n",
    "# lambdarank, aliases: objective_type, app, application\n",
    "objective  = 'binary'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse, root square loss, aliases: root_mean_squared_error, l2_root\n",
    "# - quantile, Quantile regression\n",
    "# - mape, MAPE loss, aliases: mean_absolute_percentage_error\n",
    "# - huber, Huber loss\n",
    "# - fair, Fair loss\n",
    "# - poisson, negative log-likelihood for Poisson regression\n",
    "# - gamma, negative log-likelihood for Gamma regression\n",
    "# - gamma_deviance, residual deviance for Gamma regression\n",
    "# - tweedie, negative log-likelihood for Tweedie regression\n",
    "# - ndcg, NDCG, aliases: lambdarank\n",
    "# - map, MAP, aliases: mean_average_precision\n",
    "# - auc, AUC\n",
    "# - binary_logloss, log loss, aliases: binary\n",
    "metric = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[lightGBM params](https://lightgbm.readthedocs.io/en/latest/Parameters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'learning_rate' : eta,\n",
    "    'min_data_in_leaf' : min_data_in_leaf,\n",
    "    'max_depth' : max_depth,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'bagging_freq' : bagging_freq,\n",
    "    'reg_lambda' : reg_lambda,\n",
    "    'reg_alpha' : reg_alpha,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.01],\n",
       " 'min_data_in_leaf': [20,\n",
       "  60,\n",
       "  100,\n",
       "  140,\n",
       "  180,\n",
       "  220,\n",
       "  260,\n",
       "  300,\n",
       "  340,\n",
       "  380,\n",
       "  420,\n",
       "  460,\n",
       "  500,\n",
       "  540,\n",
       "  580,\n",
       "  620,\n",
       "  660,\n",
       "  700,\n",
       "  740,\n",
       "  780,\n",
       "  820,\n",
       "  860,\n",
       "  900,\n",
       "  940,\n",
       "  980],\n",
       " 'max_depth': [3, 5, 7, 9],\n",
       " 'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       " 'subsample': [0.6, 0.7, 0.8, 0.9],\n",
       " 'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
       " 'bagging_freq': [3, 5, 10, 20, 30],\n",
       " 'reg_lambda': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
       " 'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find num boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "lgb_params = {\n",
    "    'boosting_type': boosting,\n",
    "    'objective': objective,\n",
    "    'metric': metric,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_threads' : 8,\n",
    "    'verbose': 0,\n",
    "    #'num_class':  NCLASS,\n",
    "    'seed' : SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = lgb.cv(lgb_params, lgb_train, num_boost_round = 1000, nfold = CV, metrics = metric, early_stopping_rounds = early_stopping_rounds, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = pd.DataFrame(cvresult).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators found:  285\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of estimators found: \", n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model = LGBMClassifier(n_estimators=n_estimators, \n",
    "                       #num_classes=NCLASS, \n",
    "                       scale_pos_weight=scale_pos_weight, \n",
    "                       #is_unbalance = True,\n",
    "                       objective=objective, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using CV fold cross validation, \n",
    "# search across NITER different combinations, and use all available cores\n",
    "gbm_rsearch = RandomizedSearchCV(estimator = model, param_distributions = random_grid, scoring=SCORE, n_iter = NITER, cv = CV, verbose=2, random_state=SEED, n_jobs = NJOBS)# Fit the random search model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 201902 **********\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[50]\ttraining's l2: 3929.36\ttraining's l1: 19.7834\tvalid_1's l2: 4914.17\tvalid_1's l1: 19.9251\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's l2: 3836.62\ttraining's l1: 19.6415\tvalid_1's l2: 4905.82\tvalid_1's l1: 19.9104\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cc0b55904b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"mae\",\n\u001b[1;32m     16\u001b[0m                 eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=50)\n\u001b[0;32m---> 17\u001b[0;31m     test_preds.append(pd.Series(learner.predict(X_test.drop(drop_cols, axis=1)),\n\u001b[0m\u001b[1;32m     18\u001b[0m                                 index=X_test.index, name=\"fold_\" + str(mes)))\n\u001b[1;32m     19\u001b[0m     train_preds.append(pd.Series(learner.predict(Xv.drop(drop_cols, axis=1)),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "drop_cols = [\"codmes\"]\n",
    "test_preds = []\n",
    "train_preds = []\n",
    "y_train[\"target\"] = y_train[\"margen\"].astype(\"float32\")\n",
    "for mes in X_train.codmes.unique():\n",
    "    print(\"*\"*10, mes, \"*\"*10)\n",
    "    Xt = X_train[X_train.codmes != mes]\n",
    "    yt = y_train.loc[Xt.index, \"target\"]\n",
    "    Xt = Xt.drop(drop_cols, axis=1)\n",
    "\n",
    "    Xv = X_train[X_train.codmes == mes]\n",
    "    yv = y_train.loc[Xv.index, \"target\"]\n",
    "    \n",
    "    learner = LGBMRegressor(n_estimators=1000, n_jobs=-1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"mae\",\n",
    "                eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=50)\n",
    "    test_preds.append(pd.Series(learner.predict(X_test.drop(drop_cols, axis=1)),\n",
    "                                index=X_test.index, name=\"fold_\" + str(mes)))\n",
    "    train_preds.append(pd.Series(learner.predict(Xv.drop(drop_cols, axis=1)),\n",
    "                                index=Xv.index, name=\"probs\"))\n",
    "\n",
    "test_preds = pd.concat(test_preds, axis=1).mean(axis=1)\n",
    "train_preds = pd.concat(train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xgboostenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgboostenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgboostenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgboostenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgboostenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(gbm_rsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.to_csv('output/results/rsearch_gbm_classifier_d' + str(datetime.now().date()) + '.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_freq=10, boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=0.9, gamma=0.2, importance_type='split',\n",
       "        learning_rate=0.01, max_depth=9, metric='auc',\n",
       "        min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=140,\n",
       "        min_split_gain=0.0, n_estimators=285, n_jobs=-1, num_leaves=31,\n",
       "        objective='binary', random_state=None, reg_alpha=0.001,\n",
       "        reg_lambda=0.4, scale_pos_weight=9.22705413575158, silent=True,\n",
       "        subsample=0.7, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_rsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'reg_lambda': 0.4,\n",
       " 'reg_alpha': 0.001,\n",
       " 'min_data_in_leaf': 140,\n",
       " 'max_depth': 9,\n",
       " 'learning_rate': 0.01,\n",
       " 'gamma': 0.2,\n",
       " 'colsample_bytree': 0.9,\n",
       " 'bagging_freq': 10}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_rsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  :  0.7941493817885061\n"
     ]
    }
   ],
   "source": [
    "print(SCORE,' : ', gbm_rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/models/rseach_gbm_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', gbm_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/models/rseach_gbm_classifier_best_estimator_d' + str(datetime.now().date()) + '.npy', gbm_rsearch.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xgboost (env)",
   "language": "python",
   "name": "xgboostenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
