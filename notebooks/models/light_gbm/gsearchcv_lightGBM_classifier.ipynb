{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LigthGBM CLASSIFIER - GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '../data/train_test/'\n",
    "SEED = 47\n",
    "NITER = 100\n",
    "CV = 5\n",
    "SCORE = 'roc_auc' # chouse the score in base what you want to predict\n",
    "usenull = True\n",
    "NJOBS = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load(DATAPATH+'X_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68893, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_svd300 = np.load(DATAPATH+'X_train_tfidf_svd300.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68893, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_svd300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.concatenate((train_features, train_tfidf_svd300), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(DATAPATH+'y_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if usenull == False:\n",
    "    train_features[np.isnan(train_features)] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "boosting = 'gbdt'\n",
    "\n",
    "# Learning Task Parameters\n",
    "# This defines the loss function to be minimized. See documentation\n",
    "# -  options: regression, regression_l1, huber, fair, poisson, quantile, \n",
    "# mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda,\n",
    "# lambdarank, aliases: objective_type, app, application\n",
    "objective  = 'binary'\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse, root square loss, aliases: root_mean_squared_error, l2_root\n",
    "# - quantile, Quantile regression\n",
    "# - mape, MAPE loss, aliases: mean_absolute_percentage_error\n",
    "# - huber, Huber loss\n",
    "# - fair, Fair loss\n",
    "# - poisson, negative log-likelihood for Poisson regression\n",
    "# - gamma, negative log-likelihood for Gamma regression\n",
    "# - gamma_deviance, residual deviance for Gamma regression\n",
    "# - tweedie, negative log-likelihood for Tweedie regression\n",
    "# - ndcg, NDCG, aliases: lambdarank\n",
    "# - map, MAP, aliases: mean_average_precision\n",
    "# - auc, AUC\n",
    "# - binary_logloss, log loss, aliases: binary\n",
    "metric = 'auc'\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = int((len(train_labels) - np.sum(train_labels))/np.sum(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load  hyperparameters from random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = np.load('../models/rseach_gbm_classifier_bestparams_d2019-10-30.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.8,\n",
       " 'reg_lambda': 0.5,\n",
       " 'reg_alpha': 0.001,\n",
       " 'min_data_in_leaf': 180,\n",
       " 'max_depth': 9,\n",
       " 'learning_rate': 0.01,\n",
       " 'gamma': 0.4,\n",
       " 'colsample_bytree': 0.7,\n",
       " 'bagging_freq': 20}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params['seed'] = SEED\n",
    "lgb_params['boosting_type'] = boosting\n",
    "lgb_params['objective'] = objective\n",
    "lgb_params['metric'] = metric\n",
    "lgb_params['num_threads'] = NJOBS\n",
    "lgb_params['verbose'] = 0\n",
    "lgb_params['scale_pos_weight'] = scale_pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find num boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = lgb.cv(lgb_params, lgb_train, num_boost_round = 1000, nfold = CV, metrics = metric, early_stopping_rounds = early_stopping_rounds, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = pd.DataFrame(cvresult).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators found:  999\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of estimators found: \", n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "boosting = 'gbdt'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = [0.01] \n",
    "\n",
    "# Minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "# Default 1e-3\n",
    "min_child_weight = [0.001]\n",
    "\n",
    "# Minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "# default = 20, type = int, aliases: min_data_per_leaf, min_data, min_child_samples, \n",
    "# constraints: min_data_in_leaf >= 0\n",
    "min_data_in_leaf = [i for i in range(int(lgb_params['min_data_in_leaf']) - 10, int(lgb_params['min_data_in_leaf']) + 12, 4)]\n",
    "\n",
    "# The maximum depth of a tree\n",
    "max_depth = [i for i in range(int(lgb_params['max_depth']) - 1, int(lgb_params['max_depth']) + 2, 1)]\n",
    "\n",
    "# A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "# Gamma specifies the minimum loss reduction required to make a split.\n",
    "gamma = [lgb_params['gamma']]\n",
    "\n",
    "# Denotes the fraction of observations to be randomly samples for each tree.\n",
    "subsample = [i/100 for i in range(int(lgb_params['subsample']*100) - 4, int(lgb_params['subsample']*100) + 6, 3)]\n",
    "\n",
    "# Denotes the fraction of columns to be randomly samples for each tree.\n",
    "colsample_bytree = [i/100 for i in range(int(lgb_params['colsample_bytree']*100) - 4, int(lgb_params['colsample_bytree']*100) + 6, 3)]\n",
    "\n",
    "# frequency for bagging\n",
    "# 0 means disable bagging; k means perform bagging at every k iteration\n",
    "# Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n",
    "bagging_freq = [lgb_params['bagging_freq']]\n",
    "\n",
    "# L2 regularization term on weights (analogous to Ridge regression)\n",
    "reg_lambda = [i/100 for i in range(int(lgb_params['reg_lambda']*100) - 4, int(lgb_params['reg_lambda']*100) + 4, 4)]\n",
    "\n",
    "# L1 regularization term on weight (analogous to Lasso regression)\n",
    "reg_alpha = [lgb_params['reg_alpha']]\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = int((len(train_labels) - np.sum(train_labels))/np.sum(train_labels))\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "# This defines the loss function to be minimized. See documentation\n",
    "# -  options: regression, regression_l1, huber, fair, poisson, quantile, \n",
    "# mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda,\n",
    "# lambdarank, aliases: objective_type, app, application\n",
    "objective  = 'binary'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse, root square loss, aliases: root_mean_squared_error, l2_root\n",
    "# - quantile, Quantile regression\n",
    "# - mape, MAPE loss, aliases: mean_absolute_percentage_error\n",
    "# - huber, Huber loss\n",
    "# - fair, Fair loss\n",
    "# - poisson, negative log-likelihood for Poisson regression\n",
    "# - gamma, negative log-likelihood for Gamma regression\n",
    "# - gamma_deviance, residual deviance for Gamma regression\n",
    "# - tweedie, negative log-likelihood for Tweedie regression\n",
    "# - ndcg, NDCG, aliases: lambdarank\n",
    "# - map, MAP, aliases: mean_average_precision\n",
    "# - auc, AUC\n",
    "# - binary_logloss, log loss, aliases: binary\n",
    "metric = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[lightGBM params](https://lightgbm.readthedocs.io/en/latest/Parameters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "param_grid = {\n",
    "    'learning_rate' : eta,\n",
    "    'min_data_in_leaf' : min_data_in_leaf,\n",
    "    'max_depth' : max_depth,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'bagging_freq' : bagging_freq,\n",
    "    'reg_lambda' : reg_lambda,\n",
    "    'reg_alpha' : reg_alpha,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.01],\n",
       " 'min_data_in_leaf': [170, 174, 178, 182, 186, 190],\n",
       " 'max_depth': [8, 9, 10],\n",
       " 'gamma': [0.4],\n",
       " 'subsample': [0.76, 0.79, 0.82, 0.85],\n",
       " 'colsample_bytree': [0.66, 0.69, 0.72, 0.75],\n",
       " 'bagging_freq': [20],\n",
       " 'reg_lambda': [0.46, 0.5],\n",
       " 'reg_alpha': [0.001]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model = LGBMClassifier(n_estimators=n_estimators, scale_pos_weight=scale_pos_weight, objective=objective, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using CV fold cross validation, \n",
    "# search across NITER different combinations, and use all available cores\n",
    "gbm_gsearch = GridSearchCV(estimator = model, param_grid=param_grid, scoring=SCORE, cv = CV, verbose=2, n_jobs = NJOBS)# Fit the random search model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=28)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=28)]: Done 106 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=28)]: Done 309 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=28)]: Done 592 tasks      | elapsed: 55.5min\n",
      "[Parallel(n_jobs=28)]: Done 957 tasks      | elapsed: 88.8min\n",
      "[Parallel(n_jobs=28)]: Done 1402 tasks      | elapsed: 143.4min\n",
      "[Parallel(n_jobs=28)]: Done 1929 tasks      | elapsed: 206.1min\n",
      "[Parallel(n_jobs=28)]: Done 2536 tasks      | elapsed: 267.2min\n",
      "[Parallel(n_jobs=28)]: Done 2880 out of 2880 | elapsed: 302.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 18156.24 seconds for 100 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "gbm_gsearch.fit(train_features, train_labels)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), NITER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(gbm_gsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.to_csv('../models/gsearch_gbm_classifier_d' + str(datetime.now().date()) + '.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_freq=20, boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=0.66, gamma=0.4, importance_type='split',\n",
       "               learning_rate=0.01, max_depth=9, metric='auc',\n",
       "               min_child_samples=20, min_child_weight=0.001,\n",
       "               min_data_in_leaf=170, min_split_gain=0.0, n_estimators=999,\n",
       "               n_jobs=-1, num_leaves=31, objective='binary', random_state=None,\n",
       "               reg_alpha=0.001, reg_lambda=0.5, scale_pos_weight=2, silent=True,\n",
       "               subsample=0.85, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = gbm_gsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params['seed'] = SEED\n",
    "lgb_params['boosting_type'] = boosting\n",
    "lgb_params['objective'] = objective\n",
    "lgb_params['metric'] = metric\n",
    "lgb_params['num_threads'] = NJOBS\n",
    "lgb_params['verbose'] = 0\n",
    "lgb_params['scale_pos_weight'] = scale_pos_weight\n",
    "lgb_params['n_estimators'] = n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_freq': 20,\n",
       " 'colsample_bytree': 0.66,\n",
       " 'gamma': 0.4,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 9,\n",
       " 'min_data_in_leaf': 170,\n",
       " 'reg_alpha': 0.001,\n",
       " 'reg_lambda': 0.5,\n",
       " 'subsample': 0.85,\n",
       " 'seed': 47,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'num_threads': 28,\n",
       " 'verbose': 0,\n",
       " 'scale_pos_weight': 2,\n",
       " 'n_estimators': 999}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  :  0.7795858668857277\n"
     ]
    }
   ],
   "source": [
    "print(SCORE,' : ', gbm_gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../models/gseach_gbm_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../models/gseach_gbm_classifier_best_estimator_d' + str(datetime.now().date()) + '.npy', gbm_gsearch.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
