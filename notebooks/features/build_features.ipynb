{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../../data/processed'\n",
    "OUTPUT_PATH = '../../data/features'\n",
    "OUTPUT_FILE_NAME = 'features_v006'\n",
    "INPUT_FILE_NAME = 'dataproc_v005'\n",
    "DAYS_PRED = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demand_features(df):\n",
    "    for diff in [0, 1, 2, 3, 4, 5]:\n",
    "        shift = DAYS_PRED + diff\n",
    "        df[f\"demand_shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n",
    "        )\n",
    "\n",
    "    # Moving average\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n",
    "        )\n",
    "    \n",
    "    # Moving Acum average\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_acum_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).expanding(min_periods=window).mean()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).min()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).max()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_skew_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).skew()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_rolling_kurt_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).kurt()\n",
    "        )\n",
    "\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demand_smooth_features(df):\n",
    "    for diff in [0, 1, 2, 3, 4, 5]:\n",
    "        shift = DAYS_PRED + diff\n",
    "        df[f\"demand_smoothed_shift_t{shift}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n",
    "        )\n",
    "\n",
    "    # Moving average\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n",
    "        )\n",
    "    \n",
    "    # Moving Acum average\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_acum_mean_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).expanding(min_periods=window).mean()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).min()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).max()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_skew_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).skew()\n",
    "        )\n",
    "\n",
    "    for window in [5, 10, 30, 70, 90, 120, 180]:\n",
    "        df[f\"demand_smoothed_rolling_kurt_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).kurt()\n",
    "        )\n",
    "    \n",
    "    for window in [10, 50, 100, 180, 360]:\n",
    "        df[f\"demand_smoothed_rolling_q10_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).quantile(0.1)\n",
    "    \n",
    "    for window in [10, 50, 100, 180, 360]:\n",
    "        df[f\"demand_smoothed_rolling_q50_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).median()\n",
    "    \n",
    "    for window in [10, 50, 100, 180, 360]:\n",
    "        df[f\"demand_smoothed_rolling_q90_t{window}\"] = df.groupby([\"id\"])[\"demand_smoothed\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).quantile(0.9)\n",
    "        )\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_price_features(df):\n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "\n",
    "    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    attrs = [\n",
    "        \"year\",\n",
    "        \"quarter\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading files...\")\n",
    "data = pd.read_pickle(f'{INPUT_PATH}/{INPUT_FILE_NAME}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_demand_features(data).pipe(reduce_mem_usage)\n",
    "data = add_demand_smooth_features(data).pipe(reduce_mem_usage)\n",
    "data = add_price_features(data).pipe(reduce_mem_usage)\n",
    "dt_col = \"date\"\n",
    "data = add_time_features(data, dt_col).pipe(reduce_mem_usage)\n",
    "data = data.sort_values(\"date\")\n",
    "\n",
    "print(\"start date:\", data[dt_col].min())\n",
    "print(\"end date:\", data[dt_col].max())\n",
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(f'{OUTPUT_PATH}/{OUTPUT_FILE_NAME}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(['id','date'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.id == 'FOODS_1_001_CA_1_validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.demand_smoothed.round().astype(int).clip(0)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
